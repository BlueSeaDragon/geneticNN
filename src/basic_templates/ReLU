import torch

"""
BEGIN_PROPS
{
 "OUT": [
 "VAR:Y ___input_dim____"
 ],
 "IN": [
 "VAR:X ___input_dim____"
 ]
}
END_PROPS
"""

class ReLU(torch.nn.Module):
    """
    Applies the rectified linear unit function element-wise:
    ReLU(x) = max(0, x)

    Parameters:
    inplace (bool): can optionally do the operation in-place. Default: False

    Shape:
    Input: (∗), where ∗ means any number of dimensions.
    Output: (∗), same shape as the input.
    """
    def __init__(self, 
        ___input_dim____ = 10, 
        ___device____ = 'cpu', 
        ___dtype____ = torch.float32,
        ):
        super(ReLU, self).__init__()
        self.relu = torch.nn.ReLU(inplace = False, 
                                device=___device____, 
                                dtype=___dtype____
                                )

    def forward(self, X):
        x = X['X']
        res = {"Y":self.relu(x)}
        return res
